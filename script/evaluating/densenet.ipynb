{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "root = Path(os.getcwd()).parents[1].resolve()\n",
    "sys.path.insert(0, str(root))\n",
    "\n",
    "assert load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import dtypes, one_hot, io, image, data\n",
    "from core.utils.config import BaselineConfig, emotion\n",
    "from core.models.baseline import DenseNet\n",
    "from core.utils.config import BaselineConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model params\n",
    "densenet_config = BaselineConfig()\n",
    "\n",
    "frame_shape = (\n",
    "    densenet_config.image_height,\n",
    "    densenet_config.image_width,\n",
    "    densenet_config.image_channels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet(densenet_config)\n",
    "\n",
    "model.load_weights(root.joinpath(\"models/baseline/densenet121_rgb.h5\"))\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(path):\n",
    "    \"\"\"generate tensorflow.data.Dataset from images\n",
    "\n",
    "    Args:\n",
    "        path (Tensor): Tensor object from tensorflow.data.Dataset\n",
    "\n",
    "    Returns:\n",
    "        img: tensorflow image object\n",
    "    \"\"\"\n",
    "    # load image\n",
    "    img = io.read_file(path)\n",
    "    img = io.decode_jpeg(img, channels=3)\n",
    "    img = image.convert_image_dtype(img, dtypes.float32)\n",
    "    img = image.resize(img, frame_shape[:2])\n",
    "    img = img / 255.0\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_tf_dataset(path: os.PathLike):\n",
    "\n",
    "    image_list = list(path.rglob(\"*.jpg\"))\n",
    "\n",
    "    label_list = [emotion[path.parent.name] for path in image_list]\n",
    "\n",
    "    # image\n",
    "    image_dataset = data.Dataset.from_tensor_slices(\n",
    "        [str(path) for path in image_list]\n",
    "    ).map(process_image)\n",
    "\n",
    "    # label\n",
    "    label_dataset = data.Dataset.from_tensor_slices(label_list).map(\n",
    "        lambda x: one_hot(indices=x, depth=7, dtype=dtypes.uint8)\n",
    "    )\n",
    "\n",
    "    return data.Dataset.zip((image_dataset, label_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = root.joinpath(\"data/affectnet\")\n",
    "\n",
    "assert data_dir.exists()\n",
    "\n",
    "validation_dataset = (\n",
    "    generate_tf_dataset(data_dir.joinpath(\"test/faces\"))\n",
    "    .batch(batch_size=32)\n",
    "    .prefetch(buffer_size=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, label in validation_dataset.take(1):\n",
    "    print(batch.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(validation_dataset)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c4ebb0d6ef65895a57fba3d6def3f04f0a394049f1ae22a70ba1bd7e59f8035"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
